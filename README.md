# -*- coding: utf-8 -*-
"""alleregn detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eGYJIAJ4QbrVKNtOmfi97CNKN--7KRdP

# Initial Setup
"""

# This code mounts your Google Drive in the Colab environment
# You can access files and directories from your Google Drive account

from google.colab import drive
drive.mount('/content/gdrive')

path="/content/gdrive/MyDrive/Food Ingredient/food_ingredients_and_allergens.csv"

import pandas as pd
import numpy as np

df=pd.read_csv(path)

"""# Data Cleaning - replacing the null values, mean values"""

df.head()

df.shape

sum(df.duplicated())

df.drop_duplicates(inplace = True)
df.shape

df.isna().sum()

df.dropna(inplace = True)
df.shape

df.columns

for item in list(df.columns):
    print(item , "   " , df[item].nunique())

mask = (df['Allergens'] == 'None') & (df['Prediction'] == 'Contains')
rows_to_drop = df[mask].index
df.drop(rows_to_drop, inplace=True)

rows_to_drop

df.shape

df.reset_index(inplace=True)

"""# Data Normalisation - combining all the features into single column"""

df['text']=' '+df['Main Ingredient']+' '+df['Sweetener']+' '+df['Fat/Oil']+' '+df['Seasoning']+' '+df['Allergens']

df

df.replace('Vegetable oil', 'VegetableOil', regex=True, inplace=True)
df.replace('Olive oil', 'OliveOil', regex=True, inplace=True)

"""# Data Preprocessing - removal of puntuation, stop words, case conversion and stemming

"""

import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer,PorterStemmer
from nltk.corpus import stopwords
import re
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

import nltk
nltk.download('stopwords')

import nltk
nltk.download('wordnet')

for index, i in df['text'].iteritems():
  print(i)

for index, i in df['text'].iteritems():
  print(str(i))
  sentence = i.lower()
  sentence=sentence.replace('{html}',"")
  cleanr = re.compile('<.*?>:')
  cleantext = re.sub(cleanr, '', sentence)
  rem_url=re.sub(r'http\S+', '',cleantext)
  rem_num = re.sub('[0-9]+', '', rem_url)
  tokenizer = RegexpTokenizer(r'\w+')
  tokens = tokenizer.tokenize(rem_num)
  filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english') and "amp"]
  stem_words=[stemmer.stem(w) for w in filtered_words]
  lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]
  print(" ".join(filtered_words))
  df['text'].iloc[index] = " ".join(filtered_words)

df['text']

df

df.describe()

"""# Assigning target and features"""

target=df['Prediction']
features=df['text']

"""# Splitting train and test data"""



from sklearn.model_selection import train_test_split

features_train , features_test , target_train , target_test = train_test_split(features,target , test_size=0.2 , random_state=23, stratify=target)

"""# Feature Extraction (Bag of Words and TFIDF Vectorizer)"""

from sklearn.feature_extraction.text import CountVectorizer
bow_vectorizer = CountVectorizer()
X_train_bow = bow_vectorizer.fit_transform(features_train)
X_test_bow = bow_vectorizer.transform(features_test)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
trainf = vectorizer.fit_transform(features_train)
testf = vectorizer.transform(features_test)

"""# Random Forest

---

## using TFIDF
"""

from sklearn.ensemble import RandomForestClassifier
ran = RandomForestClassifier(max_depth=15, random_state=0)
ran.fit(trainf, target_train)
y_pred=clf.predict(testf)
from sklearn.metrics import classification_report
print(classification_report(target_test, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(target_test, y_pred)

# Create a heatmap using seaborn
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=clf.classes_,
            yticklabels=clf.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix Heatmap')
plt.show()

"""## Bag of Words(BoW)"""

from sklearn.ensemble import RandomForestClassifier
ran_bow = RandomForestClassifier(max_depth=15, random_state=0)
ran_bow.fit(X_train_bow, target_train)
ran_pred_bow=clf.predict(X_test_bow)
from sklearn.metrics import classification_report
print(classification_report(target_test, ran_pred_bow))

conf_matrix = confusion_matrix(target_test, ran_pred_bow)

# Create a heatmap using seaborn
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=ran_bow.classes_,
            yticklabels=ran_bow.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix Heatmap')
plt.show()

"""# K Nearest Neighbor

## Bag of Words
"""

from sklearn.neighbors import KNeighborsClassifier
neigh_bow = KNeighborsClassifier(n_neighbors=3)
neigh_bow.fit(X_train_bow, target_train)
neigh_pred_bow=neigh_bow.predict(X_test_bow)
from sklearn.metrics import classification_report
print(classification_report(target_test, neigh_pred_bow))

conf_matrix = confusion_matrix(target_test, neigh_pred_bow)

# Create a heatmap using seaborn
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=neigh_bow.classes_,
            yticklabels=neigh_bow.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix Heatmap')
plt.show()

"""## TF-IDF"""

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(trainf, target_train)
neigh_pred=neigh.predict(testf)
from sklearn.metrics import classification_report
print(classification_report(target_test, neigh_pred))

"""# Logistic Regression

## TF IDF
"""

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state=0).fit(trainf, target_train)
log_pred=log.predict(testf)
from sklearn.metrics import classification_report
print(classification_report(target_test, log_pred))

conf_matrix = confusion_matrix(target_test, log_pred)

# Create a heatmap using seaborn
plt.figure(figsize=(4, 2))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=log.classes_,
            yticklabels=log.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix Heatmap')
plt.show()

"""## Bag of Words"""

from sklearn.linear_model import LogisticRegression
log_bow = LogisticRegression(random_state=0).fit(X_train_bow, target_train)
log_pred_bow=log_bow.predict(X_test_bow)
from sklearn.metrics import classification_report
print(classification_report(target_test, log_pred_bow))

"""# Naive Bayes"""

## TF-IDF

from sklearn.naive_bayes import GaussianNB
trainf_dense = trainf.toarray()
testf_dense = testf.toarray()
gnb = GaussianNB()
gnb.fit(trainf_dense, target_train)
y_pred4= gnb.predict(testf_dense)

from sklearn.metrics import classification_report
print(classification_report(target_test, y_pred))

"""## Bag of Words"""

from sklearn.naive_bayes import GaussianNB
trainf_bow = X_train_bow.toarray()
testf_bow = X_test_bow.toarray()

gnb = GaussianNB()
gnb.fit(trainf_bow, target_train)
y_pred4_bow = gnb.predict(testf_bow)

from sklearn.metrics import classification_report
print(classification_report(target_test, y_pred))

"""# Optical Character Recognition"""

pip install easyocr

import easyocr

image_path = '/content/gdrive/MyDrive/Food Ingredient/image/i1.jpeg'  # Replace 'your_image_path.jpg' with the path to your image
reader = easyocr.Reader(['en'])

result = reader.readtext("/content/gdrive/MyDrive/Food Ingredient/image/i1.jpeg")
print(result)

detected_texts = [detection[1] for detection in result]

# Concatenate all the detected texts into a single string
final_result = ", ".join(detected_texts)

# Print the concatenated final result
print(final_result)

STRING = final_result

# Split the string into rows using commas as delimiters
parts = STRING.split(',')
parts = [part.split(';') for part in parts]

# Flatten the list
rows = [item for sublist in parts for item in sublist]

# Now, we will have a list where each string is separated by ',' and '-'
print(rows)

# Create a list to store the data for each row
data = []

# Loop through each row and clean the data
for row in rows:
    data.append(row.strip())

# Concatenate all the rows into a single string
combined_data = ', '.join(data)

# Create a DataFrame with a single row containing the combined data
ocr_text = pd.DataFrame([combined_data], columns=['Combined Data'])

# Display the DataFrame
print(ocr_text)

ocr_text.head()

"""# Preprocessing of extracted text data"""

import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer,PorterStemmer
from nltk.corpus import stopwords
import re
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

>>> import nltk
  >>> nltk.download('stopwords')

  >>> import nltk
  >>> nltk.download('wordnet')

def preprocess(sentence):
  sentence=str(sentence)
  sentence = sentence.lower()
  sentence=sentence.replace('{html}',"")
  cleanr = re.compile('<.*?>:')
  cleantext = re.sub(cleanr, '', sentence)
  rem_url=re.sub(r'http\S+', '',cleantext)
  rem_num = re.sub('[0-9]+', '', rem_url)
  tokenizer = RegexpTokenizer(r'\w+')
  tokens = tokenizer.tokenize(rem_num)
  filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english') and "amp"]
  stem_words=[stemmer.stem(w) for w in filtered_words]
  lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]
  return " ".join(filtered_words)
ocr_text['cleanText'] = preprocess(final_result)

ocr_text.head()

"""# Testing the model with preprocessed text data from image"""

newfeatures = vectorizer.transform(ocr_text['cleanText'])
pred=log.predict(newfeatures)
pred

pip install tabulate

import pandas as pd
from tabulate import tabulate

# Transform the new features using the same vectorizer
newfeatures = vectorizer.transform(ocr_text['cleanText'])

# Predict labels for the new features
pred = log.predict(newfeatures)

# Create a DataFrame to store the results
results_df = pd.DataFrame({
    'Input Data': ocr_text['cleanText'],  # Assuming 'cleanText' is the column with input data
    'Predicted Label': pred
})

# Convert the DataFrame to a tabulated format
tabulated_results = tabulate(results_df, headers='keys', tablefmt='pretty')

# Print the tabulated results
print(tabulated_results)
